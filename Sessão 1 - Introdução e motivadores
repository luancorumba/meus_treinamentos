{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1eOpet3ApUpHcFWWBT-EOh0YiSAVzudTV","authorship_tag":"ABX9TyOJdhJAix5ORp7N2Dc7IERr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["###### Inicialização"],"metadata":{"id":"lJeDRY2Zc6Tn"}},{"cell_type":"code","source":["# instalar as dependências\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n","!tar xf spark-3.4.0-bin-hadoop3.tgz\n","!pip install -q findspark"],"metadata":{"id":"ty_kN91oadtM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configurar as variáveis de ambiente\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\"\n","\n","# tornar o pyspark \"importável\"\n","import findspark\n","findspark.init('spark-3.4.0-bin-hadoop3')"],"metadata":{"id":"GMGdL4xyc423"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PySpark (Sessão 1)\n","\n"],"metadata":{"id":"frFI5O7cZ5L8"}},{"cell_type":"markdown","source":["![img](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/250px-Apache_Spark_logo.svg.png)\n","\n","\n","https://spark.apache.org/docs/latest/api/python/"],"metadata":{"id":"pJCODccgTiQD"}},{"cell_type":"markdown","source":["- https://towardsdatascience.com/apache-spark-multi-part-series-spark-architecture-461d81e24010\n","- https://www.oreilly.com/library/view/data-algorithms-with/9781492082378/ch01.html\n"],"metadata":{"id":"bfZwp7wAfm3O"}},{"cell_type":"markdown","source":["## Apresentação do curso"],"metadata":{"id":"8_LqjNoYIUSS"}},{"cell_type":"markdown","source":["### Introdução ao curso de PySpark"],"metadata":{"id":"1pU-GwX2JGhC"}},{"cell_type":"markdown","source":["### Apresentação do conteúdo programático"],"metadata":{"id":"Ci3OqKEjIoJM"}},{"cell_type":"markdown","source":["- Módulo 1: Conceitos fundamentais do PySpark\n","- Módulo 2: Aprendizado de Máquina com PySpark\n","- Módulo 3: Integração de ferramentas"],"metadata":{"id":"EqCyTr2ZlbMz"}},{"cell_type":"markdown","source":["### Objetivos do curso"],"metadata":{"id":"9UKxOmPnI--O"}},{"cell_type":"markdown","source":["Torna-los fluentes no uso da biblioteca python para o Spark, em aplicações de alta volumetria de dados."],"metadata":{"id":"Ejy1hnjOSzyD"}},{"cell_type":"markdown","source":["### Pré-requisitos e conhecimentos necessários"],"metadata":{"id":"7V7wDlGKJB0A"}},{"cell_type":"markdown","source":["1. Conceitos básicos de computação\n","2. Programação em Python\n","3. Ferramental de Machine Learning"],"metadata":{"id":"5JJAccoxSaCA"}},{"cell_type":"markdown","source":["## Introdução ao PySpark"],"metadata":{"id":"ANQ5XzKwInQK"}},{"cell_type":"markdown","source":["### O que é PySpark e qual a sua importância"],"metadata":{"id":"JEm8aNCSJUXI"}},{"cell_type":"markdown","source":["O PySpark é uma biblioteca de código aberto, desenvolvida em Python, que fornece uma interface para o Apache Spark. O Apache Spark é um mecanismo de processamento de dados em larga escala, projetado para ser rápido e fácil de usar. Ele permite processar grandes volumes de dados de forma eficiente, utilizando clusters de computadores para distribuir e paralelizar o processamento.\n","\n","<img src=\"https://www.oreilly.com/api/v2/epubs/9781492082378/files/assets/daws_0103.png\" width=\"920px\"/>"],"metadata":{"id":"LebMVaEO4QEq"}},{"cell_type":"markdown","source":["### Vantagens e desvantagens do PySpark"],"metadata":{"id":"iTtzcPiXIoYG"}},{"cell_type":"markdown","source":["#### Vantagens"],"metadata":{"id":"bxGwqIj2FAZs"}},{"cell_type":"markdown","source":["\n","1. **Facilidade de uso**: O PySpark oferece uma interface Python de alto nível, facilitando o uso e a compreensão do Spark para programadores Python. Python é uma linguagem amplamente utilizada para análise de dados e machine learning, então o PySpark se torna uma opção popular para quem já tem familiaridade com essa linguagem.\n","\n","2. **Escalabilidade**: O PySpark permite que os desenvolvedores escalem facilmente suas aplicações, processando grandes volumes de dados distribuídos em clusters. Isso é especialmente útil em cenários de big data, onde o processamento tradicional de dados em uma única máquina pode ser lento ou inviável.\n","\n","3. **Desempenho**: O Apache Spark é conhecido por seu desempenho superior em comparação a outros frameworks de processamento de dados, como o Hadoop MapReduce. O PySpark aproveita esse desempenho, proporcionando um processamento mais rápido de grandes volumes de dados.\n","\n","4. **Flexibilidade**: O PySpark suporta diversos formatos de dados e fontes, como Parquet, Avro, JSON, Hadoop Distributed File System (HDFS) e Apache HBase, facilitando a integração com diferentes tipos de infraestruturas e aplicações.\n","\n","5. **Recursos avançados**: O PySpark oferece uma ampla gama de recursos avançados, como processamento de dados em tempo real (streaming), machine learning (através da biblioteca MLlib) e análise de grafos (através da biblioteca GraphX). Esses recursos permitem aos desenvolvedores criar soluções sofisticadas para análise e processamento de dados.\n","\n","6. **Comunidade e suporte**: O PySpark é apoiado por uma comunidade ativa e crescente, o que significa que os desenvolvedores podem contar com uma ampla gama de recursos, como documentação, exemplos e suporte da comunidade para solucionar problemas e melhorar suas habilidades."],"metadata":{"id":"ZAerNRDvFGTl"}},{"cell_type":"markdown","source":["#### Desvantagens"],"metadata":{"id":"FS2_iVl3FI5_"}},{"cell_type":"markdown","source":["1. **Overhead do Python**: O PySpark usa a linguagem Python, que é uma linguagem interpretada e tipicamente mais lenta do que as linguagens compiladas, como Java ou Scala. Isso pode levar a um overhead no desempenho em comparação com o uso direto do Apache Spark com essas linguagens. No entanto, muitas vezes essa diferença de desempenho é compensada pela facilidade de uso e desenvolvimento mais rápido proporcionado pelo Python.\n","\n","2. **Curva de aprendizado**: Embora o PySpark seja mais fácil de usar para desenvolvedores familiarizados com Python, ainda há uma curva de aprendizado associada ao entendimento do modelo de programação distribuída do Spark, sua arquitetura e suas abstrações de dados, como RDDs (Resilient Distributed Datasets) e DataFrames.\n","\n","3. **Gerenciamento de recursos**: O gerenciamento de recursos em clusters de Spark pode ser complexo e desafiador, especialmente para usuários novatos. A configuração e a sintonização corretas dos recursos do cluster são essenciais para garantir o desempenho e a eficiência ideais, o que pode exigir experiência e conhecimento especializado."],"metadata":{"id":"9zeICuwmE56T"}},{"cell_type":"markdown","source":["### Arquitetura do PySpark"],"metadata":{"id":"50eRC2rdJOS0"}},{"cell_type":"markdown","source":["![img](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/11/cluster-overview.png)"],"metadata":{"id":"A2xUuOALKw2Y"}},{"cell_type":"markdown","source":["A arquitetura do PySpark é baseada na arquitetura do Apache Spark, um mecanismo de processamento de dados distribuído e paralelo. O PySpark é uma interface Python para o Apache Spark, permitindo que os desenvolvedores Python interajam com o Spark e criem aplicações distribuídas. A arquitetura do PySpark pode ser dividida em várias camadas:\n","\n","1. **Camada de linguagem**: O PySpark fornece uma API em Python para interagir com o Apache Spark. A API do PySpark é projetada para ser fácil de usar e se assemelha a outras bibliotecas de processamento de dados em Python, como o Pandas.\n","\n","2. **Camada de abstração de dados**: O PySpark oferece abstrações de dados de alto nível, como RDD (Resilient Distributed Dataset) e DataFrame, que facilitam a manipulação de dados distribuídos em um cluster. O RDD é uma coleção imutável e distribuída de objetos que pode ser processada em paralelo, enquanto o DataFrame é uma abstração tabular com um esquema que permite realizar operações relacionais e de agregação de maneira mais eficiente.\n","\n","3. **Camada de execução**: A camada de execução do PySpark gerencia a distribuição e a execução de tarefas nos nós do cluster. A execução das tarefas é baseada no modelo de computação paralela e distribuída do Apache Spark, que divide os dados e as tarefas em partições e as distribui entre os executores nos nós do cluster. Os executores são processos Java que executam as tarefas e armazenam dados na memória ou no disco.\n","\n","4. **Camada de armazenamento**: O PySpark suporta várias opções de armazenamento de dados, incluindo Hadoop Distributed File System (HDFS), Apache HBase, Apache Cassandra, Amazon S3 e outros sistemas de armazenamento compatíveis. Os dados podem ser lidos e escritos em diferentes formatos, como Parquet, Avro, JSON e CSV.\n","\n","5. **Camada de gerenciamento de cluster**: O Apache Spark e o PySpark podem ser executados em diferentes gerenciadores de cluster, como standalone, Apache Mesos, Apache Hadoop YARN e Kubernetes. Esses gerenciadores de cluster são responsáveis por alocar e gerenciar os recursos do cluster, como CPU, memória e armazenamento, e garantir a execução eficiente das tarefas.\n","\n","6. **Camada de bibliotecas**: O PySpark inclui várias bibliotecas para tarefas avançadas de análise de dados e machine learning, como MLlib (para machine learning), GraphX (para processamento de grafos) e Structured Streaming (para processamento de fluxo de dados em tempo real).\n","\n","Em resumo, a arquitetura do PySpark é uma combinação de várias camadas que facilitam a criação e a execução de aplicações distribuídas em Python, aproveitando os recursos e o desempenho do Apache Spark. Essas camadas incluem a interface de linguagem, abstrações de dados, execução, armazenamento, gerenciamento de cluster e bibliotecas adicionais para análise avançada e machine learning."],"metadata":{"id":"86EXys-uGjqW"}},{"cell_type":"markdown","source":["### Funcionamento do PySpark em um cluster"],"metadata":{"id":"B1U9czzBJQoE"}},{"cell_type":"markdown","source":["O PySpark funciona em um cluster utilizando a arquitetura e os componentes do Apache Spark para distribuir e executar tarefas de processamento de dados em paralelo nos nós do cluster. Vamos examinar como o PySpark opera em um cluster, passo a passo:\n","\n","1. **Configuração do cluster**: Primeiramente, é necessário configurar um cluster com um gerenciador de cluster, como standalone, Apache Mesos, Apache Hadoop YARN ou Kubernetes. Esses gerenciadores são responsáveis por alocar e gerenciar os recursos do cluster, como CPU, memória e armazenamento.\n","\n","2. **Aplicação PySpark**: A aplicação PySpark é escrita em Python, utilizando a API do PySpark para definir as transformações e ações a serem executadas nos dados. A aplicação também pode incluir o uso de bibliotecas adicionais, como MLlib para machine learning ou GraphX para análise de grafos.\n","\n","3. **Inicialização do contexto Spark**: Quando a aplicação PySpark é iniciada, ela cria um objeto SparkContext ou SparkSession, que estabelece a conexão com o cluster e coordena a execução das tarefas. O contexto Spark é responsável por converter as transformações e ações definidas na aplicação PySpark em um plano de execução distribuído.\n","\n","4. **Divisão de dados e tarefas**: O Spark divide os dados em partições, que são distribuídas pelos nós do cluster. As tarefas de processamento são então divididas em estágios, que são conjuntos de tarefas que podem ser executadas em paralelo. Cada estágio é composto por várias tarefas, que operam em partições de dados individuais.\n","\n","5. **Distribuição de tarefas**: O gerenciador de cluster aloca recursos e lança executores nos nós do cluster. Os executores são processos Java que executam tarefas e armazenam dados na memória ou no disco. O SparkContext ou SparkSession coordena a distribuição das tarefas aos executores, com base no plano de execução distribuído.\n","\n","6. **Execução de tarefas**: Os executores executam as tarefas em paralelo, processando as partições de dados atribuídas a eles. O processamento de cada tarefa é realizado utilizando a API PySpark, e o código Python é executado através de um processo Python separado, chamado Py4J, que se comunica com o executor Java.\n","\n","7. **Troca de dados e sincronização**: Quando necessário, os dados são trocados entre os executores para realizar operações de shuffle, como a redistribuição de partições de dados com base em chaves específicas. A comunicação e a sincronização entre os executores e o driver são gerenciadas pelo SparkContext ou SparkSession.\n","\n","8. **Agregação e retorno de resultados**: Após a conclusão das tarefas, os resultados são agregados e retornados ao driver, que os coleta e os apresenta ao usuário ou os armazena em um sistema de armazenamento, como HDFS, S3 ou outro sistema compatível.\n","\n","Em resumo, o PySpark funciona em um cluster aproveitando a arquitetura distribuída e paralela do Apache Spark. Ele divide os dados e as tarefas em partições e estágios, distribui e executa as tarefas nos executores do cluster, realiza a troca de dados quando necessário"],"metadata":{"id":"8zzHnJNZMXMx"}},{"cell_type":"markdown","source":["## Primeiros passos com PySpark"],"metadata":{"id":"lnZu8sHrIniG"}},{"cell_type":"markdown","source":["### Criação de um objeto SparkContext"],"metadata":{"id":"k_0m6PhrIotr"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","sc = SparkSession.builder.master('local[*]').getOrCreate()"],"metadata":{"id":"2IST7t7eLDeh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Criação de RDDs (Resilient Distributed Datasets)"],"metadata":{"id":"cMQKdCDGLEQQ"}},{"cell_type":"code","source":["# download do http para arquivo local\n","!wget --quiet --show-progress http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2023-03-28/visualisations/listings.csv"],"metadata":{"id":"LXDZTgh9LKoF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682470177065,"user_tz":180,"elapsed":560,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"4fa585c0-050a-4b39-e8f2-238ba478cc4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\rlistings.csv.1        0%[                    ]       0  --.-KB/s               \rlistings.csv.1      100%[===================>]   4.37M  26.2MB/s    in 0.2s    \n"]}]},{"cell_type":"code","source":["airbnb_rj = sc.read.csv(\"./listings.csv\", inferSchema=True, header=True).rdd\n","airbnb_rj"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jnnp8ZEq3aYj","executionInfo":{"status":"ok","timestamp":1682470204883,"user_tz":180,"elapsed":14549,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"f65fb237-2c7d-45c5-c6e6-5fa8bf3758d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MapPartitionsRDD[14] at javaToPython at NativeMethodAccessorImpl.java:0"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["airbnb_rj.take(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQm8aHfL3p5U","executionInfo":{"status":"ok","timestamp":1682470206194,"user_tz":180,"elapsed":1473,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"24b3dd62-ac36-4cb6-8436-67201a1dfe89"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(id='17878', name='Very Nice 2Br in Copacabana w. balcony, fast WiFi', host_id='68997', host_name='Matthias', neighbourhood_group=None, neighbourhood='Copacabana', latitude='-22.96599', longitude='-43.1794', room_type='Entire home/apt', price='350', minimum_nights='5', number_of_reviews='288', last_review='2023-03-01', reviews_per_month='1.86', calculated_host_listings_count='1', availability_365='264', number_of_reviews_ltm=19.0, license=None)]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["### Operações básicas em RDDs (map, filter, reduce, etc.)\n"],"metadata":{"id":"jlijRsIyLG3e"}},{"cell_type":"markdown","source":["<img src=\"https://www.oreilly.com/api/v2/epubs/9781492082378/files/assets/daws_0101.png\" width=\"920px\"/>"],"metadata":{"id":"Fjs-_fjDUBBz"}},{"cell_type":"code","source":["airbnb_rj.map(lambda x : x[9]).take(1)"],"metadata":{"id":"_Y5BiwkhLLXx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682470289582,"user_tz":180,"elapsed":519,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"d53f8b46-381b-4dad-afee-ec1d70588586"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['350']"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["airbnb_rj.map(lambda x : x[9]).filter(lambda x: (x != None) and (not x.isnumeric())).collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tuJr0XTwAxwb","executionInfo":{"status":"ok","timestamp":1682470339465,"user_tz":180,"elapsed":6098,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"421b9847-f80a-4986-ba40-1b8e39177cae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Entire home/apt',\n"," 'Entire home/apt',\n"," '-22.9829',\n"," 'Entire home/apt',\n"," '-22.98346',\n"," 'Entire home/apt',\n"," 'Entire home/apt',\n"," 'Entire home/apt',\n"," 'Entire home/apt',\n"," 'Entire home/apt',\n"," '-43.19512939453125',\n"," 'Entire home/apt',\n"," 'Entire home/apt',\n"," 'Entire home/apt',\n"," 'Entire home/apt',\n"," 'Entire home/apt']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["precos_rj = airbnb_rj.map(lambda x : x[9]).filter(lambda x: (x != None) and (x.isnumeric()))\n","precos_rj.take(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbgvN4PZB9OP","executionInfo":{"status":"ok","timestamp":1682470404072,"user_tz":180,"elapsed":1149,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"67ac5d63-49a9-4149-8b67-88bd52130696"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['350', '624', '100', '236', '307']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["precos_rj = precos_rj.map(lambda x : int(x))\n","precos_rj.take(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_s1R4IR4AVZV","executionInfo":{"status":"ok","timestamp":1682470410183,"user_tz":180,"elapsed":323,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"ff1aa78a-90e3-492f-ba0c-6e887f10c9f5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[350, 624, 100, 236, 307]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["from operator import add\n","precos_rj.reduce(add)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxwXtvVe6UM2","executionInfo":{"status":"ok","timestamp":1682470435335,"user_tz":180,"elapsed":1863,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"146b2962-c14c-4f40-e4ad-a4af983c0a39"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["29646920"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["## Conclusão da sessão\n"],"metadata":{"id":"DU8B-dAsIn4w"}},{"cell_type":"markdown","source":["1. Apresentação do Curso\n","2. Introdução ao PySpark\n","3. Primeiros Passos com PySpark"],"metadata":{"id":"AJWCPH7u6KIV"}}]}