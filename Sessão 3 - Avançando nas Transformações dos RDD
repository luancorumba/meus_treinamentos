{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["lJeDRY2Zc6Tn"],"authorship_tag":"ABX9TyNB+VPDXxPfLmRgDAHmihOo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["###### Inicialização"],"metadata":{"id":"lJeDRY2Zc6Tn"}},{"cell_type":"code","source":["# instalar as dependências\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n","!tar xf spark-3.4.0-bin-hadoop3.tgz\n","!pip install -q findspark"],"metadata":{"id":"ty_kN91oadtM","executionInfo":{"status":"ok","timestamp":1682474550690,"user_tz":180,"elapsed":14648,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# configurar as variáveis de ambiente\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\"\n","\n","# tornar o pyspark \"importável\"\n","import findspark\n","findspark.init('spark-3.4.0-bin-hadoop3')"],"metadata":{"id":"GMGdL4xyc423","executionInfo":{"status":"ok","timestamp":1682474550696,"user_tz":180,"elapsed":30,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# download do http para arquivo local\n","!wget --quiet --show-progress http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2023-03-28/visualisations/listings.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRro-0ZO38tf","executionInfo":{"status":"ok","timestamp":1682474551812,"user_tz":180,"elapsed":1136,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"43e8a94b-eac0-457a-8cc7-a573194792b7"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["\rlistings.csv.4        0%[                    ]       0  --.-KB/s               \rlistings.csv.4       38%[======>             ]   1.70M  8.26MB/s               \rlistings.csv.4      100%[===================>]   4.37M  18.3MB/s    in 0.2s    \n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master('local[*]').getOrCreate()\n","sc = spark.sparkContext"],"metadata":{"id":"EiplvfviPM9u","executionInfo":{"status":"ok","timestamp":1682474551813,"user_tz":180,"elapsed":17,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["# PySpark (Sessão 3)"],"metadata":{"id":"frFI5O7cZ5L8"}},{"cell_type":"markdown","source":["![img](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/250px-Apache_Spark_logo.svg.png)\n","\n","\n","https://spark.apache.org/docs/latest/api/python/"],"metadata":{"id":"pJCODccgTiQD"}},{"cell_type":"markdown","source":["## Recapitulando o que vimos antes..."],"metadata":{"id":"bGI5IDjEL5Z7"}},{"cell_type":"markdown","source":["1. Introdução aos RDDs\n","2. Operações de Transformação\n","3. Operações de Ação"],"metadata":{"id":"oD932sfTMe1-"}},{"cell_type":"markdown","source":["\n","## Transformações RDD Avançadas"],"metadata":{"id":"p3uQ5-_TYvdf"}},{"cell_type":"markdown","source":["As transformações avançadas de RDDs no PySpark incluem operações mais complexas e sofisticadas que podem ser aplicadas a um ou mais RDDs. Algumas das transformações avançadas mais comuns são:"],"metadata":{"id":"fCXPjMQsOTHG"}},{"cell_type":"markdown","source":["### [subtract(other)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.subtract.html#pyspark.RDD.subtract)\n","Retorna um novo RDD contendo os elementos do RDD original que não estão presentes no RDD other."],"metadata":{"id":"Ms8WvG07ONuj"}},{"cell_type":"code","source":["rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n","rdd2 = sc.parallelize([4, 5, 6, 7, 8])\n","result = rdd1.subtract(rdd2)"],"metadata":{"id":"hA4ATabvOu5X","executionInfo":{"status":"ok","timestamp":1682474643524,"user_tz":180,"elapsed":574,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["result.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AU2HkNAITc9h","executionInfo":{"status":"ok","timestamp":1682474663088,"user_tz":180,"elapsed":3312,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"1f434139-5087-45e2-9480-63f3ffc13711"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3]"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["### [zip(other)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.zip.html#pyspark.RDD.zip)\n","Retorna um novo RDD, onde cada elemento é uma tupla (pair) contendo elementos do RDD original e do RDD other com a mesma posição. Ambos os RDDs devem ter o mesmo número de partições e o mesmo número de elementos em cada partição."],"metadata":{"id":"ID4L9FyiOnM7"}},{"cell_type":"code","source":["rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n","rdd2 = sc.parallelize([1, 2, 3])\n","result = rdd1.zip(rdd2)"],"metadata":{"id":"SM7Jf52lOvRU","executionInfo":{"status":"ok","timestamp":1682474717141,"user_tz":180,"elapsed":255,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["result.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_6Z8YlQTotD","executionInfo":{"status":"ok","timestamp":1682474725530,"user_tz":180,"elapsed":131,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"0aff3195-75d9-47b0-a61e-373c3950dfdb"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('a', 1), ('b', 2), ('c', 3)]"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","source":["### [cogroup(other, [numPartitions])](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.cogroup.html#pyspark.RDD.cogroup)\n","Retorna um novo RDD de pares (key, (values1, values2)), onde values1 e values2 são as sequências de valores correspondentes a cada chave nos RDDs originais."],"metadata":{"id":"by8LviC8O9RG"}},{"cell_type":"code","source":["rdd1 = sc.parallelize([(1, \"a\"), (2, \"b\"), (3, \"c\")])\n","rdd2 = sc.parallelize([(1, \"x\"), (2, \"y\"), (3, \"z\")])\n","result = rdd1.cogroup(rdd2)"],"metadata":{"id":"aOOBSW0sO9RH","executionInfo":{"status":"ok","timestamp":1682474763963,"user_tz":180,"elapsed":515,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["result.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ryBvk00T5AP","executionInfo":{"status":"ok","timestamp":1682474773003,"user_tz":180,"elapsed":922,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"2ac7a5c6-ca84-446c-a0ae-a2c58c7fd817"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1,\n","  (<pyspark.resultiterable.ResultIterable at 0x7f70052e6490>,\n","   <pyspark.resultiterable.ResultIterable at 0x7f70051f9580>)),\n"," (2,\n","  (<pyspark.resultiterable.ResultIterable at 0x7f70051f9cd0>,\n","   <pyspark.resultiterable.ResultIterable at 0x7f70051f9a60>)),\n"," (3,\n","  (<pyspark.resultiterable.ResultIterable at 0x7f70051f9610>,\n","   <pyspark.resultiterable.ResultIterable at 0x7f70051f9820>))]"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["### [join(other, [numPartitions])](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.join.html#pyspark.RDD.join)\n","Retorna um novo RDD que contém todos os pares (key, (value1, value2)) com chaves em ambos os RDDs."],"metadata":{"id":"x9lFOvXrOxvV"}},{"cell_type":"code","source":["rdd1 = sc.parallelize([(1, \"a\"), (2, \"b\"), (3, \"c\")])\n","rdd2 = sc.parallelize([(1, \"x\"), (2, \"y\"), (3, \"z\")])\n","result = rdd1.join(rdd2)"],"metadata":{"id":"j3cGKOoxOxvV","executionInfo":{"status":"ok","timestamp":1682474842458,"user_tz":180,"elapsed":464,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["result.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2em8UXcULpn","executionInfo":{"status":"ok","timestamp":1682474847411,"user_tz":180,"elapsed":1449,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"b2149a30-5f59-4b87-d41a-d54f68e69b15"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, ('a', 'x')), (2, ('b', 'y')), (3, ('c', 'z'))]"]},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","source":["### [leftOuterJoin(other, [numPartitions])](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.leftOuterJoin.html#pyspark.RDD.leftOuterJoin) e [rightOuterJoin(other, [numPartitions])](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.rightOuterJoin.html#pyspark.RDD.rightOuterJoin)\n","Semelhante ao join, mas retorna todos os elementos do RDD da esquerda ou direita, mesmo que não haja uma chave correspondente no outro RDD. Os elementos sem correspondência terão um valor None na posição da tupla que corresponde ao RDD sem a chave correspondente."],"metadata":{"id":"Gx6G5leSPMRJ"}},{"cell_type":"code","source":["rdd1 = sc.parallelize([(1, \"a\"), (2, \"b\"), (3, \"c\")])\n","rdd2 = sc.parallelize([(1, \"x\"), (2, \"y\"), (4, \"z\")])\n","\n","# Exemplo de leftOuterJoin\n","result_left = rdd1.leftOuterJoin(rdd2)"],"metadata":{"id":"Eqw-622aPgTf","executionInfo":{"status":"ok","timestamp":1682474904199,"user_tz":180,"elapsed":1013,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["result_left.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxNHcElyUbpZ","executionInfo":{"status":"ok","timestamp":1682474919052,"user_tz":180,"elapsed":818,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"ce7999d4-e4f4-4fbc-f5ad-8ec296aaf649"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, ('a', 'x')), (2, ('b', 'y')), (3, ('c', None))]"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["# Exemplo de rightOuterJoin\n","result_right = rdd1.rightOuterJoin(rdd2)"],"metadata":{"id":"Bfu8woO5PyPa","executionInfo":{"status":"ok","timestamp":1682474907892,"user_tz":180,"elapsed":32,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["result_right.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mt4thCtdUg32","executionInfo":{"status":"ok","timestamp":1682474948784,"user_tz":180,"elapsed":763,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"90a9ddb8-b10b-4cae-9981-7720a8cee0f0"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(4, (None, 'z')), (1, ('a', 'x')), (2, ('b', 'y'))]"]},"metadata":{},"execution_count":69}]},{"cell_type":"markdown","source":["### [fullOuterJoin(other, [numPartitions])](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.fullOuterJoin.html#pyspark.RDD.fullOuterJoin)\n","Semelhante ao leftOuterJoin e rightOuterJoin, mas retorna todos os elementos de ambos os RDDs, mesmo que não haja chaves correspondentes. Os elementos sem correspondência terão um valor None na posição da tupla que corresponde ao RDD sem a chave correspondente."],"metadata":{"id":"YljMCpO5PX1j"}},{"cell_type":"code","source":["rdd1 = sc.parallelize([(1, \"a\"), (2, \"b\"), (3, \"c\")])\n","rdd2 = sc.parallelize([(1, \"x\"), (2, \"y\"), (4, \"z\")])\n","\n","result_full = rdd1.fullOuterJoin(rdd2)"],"metadata":{"id":"Rx2Xv5r3PHSX","executionInfo":{"status":"ok","timestamp":1682474968675,"user_tz":180,"elapsed":35,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["result_full.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QMPzTIMTUqi1","executionInfo":{"status":"ok","timestamp":1682475002235,"user_tz":180,"elapsed":1046,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"fd245c2f-3fc5-4651-931f-c824550a12bc"},"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(4, (None, 'z')), (1, ('a', 'x')), (2, ('b', 'y')), (3, ('c', None))]"]},"metadata":{},"execution_count":71}]},{"cell_type":"markdown","source":["### [cartesian(other)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.cartesian.html#pyspark.RDD.cartesian)\n","Retorna um novo RDD que é o produto cartesiano dos dois RDDs. Cada elemento do primeiro RDD será combinado com cada elemento do segundo RDD, resultando em todas as combinações possíveis de elementos dos dois RDDs."],"metadata":{"id":"P_acp8f-PnOU"}},{"cell_type":"code","source":["rdd1 = sc.parallelize([1, 2])\n","rdd2 = sc.parallelize([\"a\", \"b\"])\n","result = rdd1.cartesian(rdd2)"],"metadata":{"id":"4QvKo_U6PnOV","executionInfo":{"status":"ok","timestamp":1682475020105,"user_tz":180,"elapsed":401,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["result.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAnwpH2oU5Td","executionInfo":{"status":"ok","timestamp":1682475034932,"user_tz":180,"elapsed":606,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"cacd2f7b-f8e7-4156-b1df-81a6abf88407"},"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]"]},"metadata":{},"execution_count":73}]},{"cell_type":"markdown","source":["### [union(other)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.union.html#pyspark.RDD.union)\n","Retorna um novo RDD que contém a união de elementos do RDD original e do RDD other. Essa função não remove duplicatas."],"metadata":{"id":"z-dFjZHeRUwI"}},{"cell_type":"code","source":["rdd1 = sc.parallelize([1, 2, 3])\n","rdd2 = sc.parallelize([3, 4, 5])\n","result = rdd1.union(rdd2)"],"metadata":{"id":"g4lOnFCLRZum","executionInfo":{"status":"ok","timestamp":1682475050197,"user_tz":180,"elapsed":1573,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["result.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QvGYeKd_U-BB","executionInfo":{"status":"ok","timestamp":1682475056289,"user_tz":180,"elapsed":25,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"bb536adb-9ff1-4eb7-8d31-cf637dd9a2f5"},"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3, 3, 4, 5]"]},"metadata":{},"execution_count":75}]},{"cell_type":"markdown","source":["### [intersection(other)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.intersection.html#pyspark.RDD.intersection)\n","Retorna um novo RDD que contém a interseção de elementos do RDD original e do RDD other. Os elementos em comum entre os dois RDDs serão incluídos no RDD resultante. Esta função remove duplicatas."],"metadata":{"id":"XQjBoR1VRaAn"}},{"cell_type":"code","source":["rdd1 = sc.parallelize([1, 2, 3, 4])\n","rdd2 = sc.parallelize([3, 4, 5, 6])\n","result = rdd1.intersection(rdd2)"],"metadata":{"id":"ge2lP6QwRgL7","executionInfo":{"status":"ok","timestamp":1682475072696,"user_tz":180,"elapsed":14,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["result.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUaFnUFNVCWI","executionInfo":{"status":"ok","timestamp":1682475073403,"user_tz":180,"elapsed":401,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"7a1b3c63-d788-489d-8c40-43b099b636ce"},"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[4, 3]"]},"metadata":{},"execution_count":77}]},{"cell_type":"markdown","source":["### [sortBy(keyfunc, ascending=True, numPartitions=None)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html#pyspark.RDD.sortBy)\n","Retorna um novo RDD que contém os elementos do RDD original ordenados de acordo com a função keyfunc. A função keyfunc é aplicada a cada elemento do RDD, e o resultado dessa função é usado como a chave de classificação. O parâmetro ascending determina a ordem de classificação (crescente ou decrescente), e numPartitions especifica o número de partições no RDD resultante."],"metadata":{"id":"htNahcMsRhEr"}},{"cell_type":"code","source":["rdd = sc.parallelize([(\"apple\", 3), (\"banana\", 5), (\"orange\", 2), (\"kiwi\", 4)])\n","result = rdd.sortBy(lambda x: x[1], ascending=False)"],"metadata":{"id":"Lmpq6X3IRmTm","executionInfo":{"status":"ok","timestamp":1682475097929,"user_tz":180,"elapsed":771,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["result.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PuSlFeFVKMS","executionInfo":{"status":"ok","timestamp":1682475105452,"user_tz":180,"elapsed":442,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"999807c5-46b1-4174-93d9-1181638a0750"},"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('banana', 5), ('kiwi', 4), ('apple', 3), ('orange', 2)]"]},"metadata":{},"execution_count":79}]},{"cell_type":"markdown","source":["\n","## Estratégias de Particionamento"],"metadata":{"id":"LYUvuiOJY0iE"}},{"cell_type":"markdown","source":["Quando um RDD é criado, os dados são divididos em partições, que são unidades básicas de processamento paralelo. Cada partição é uma fração do RDD e contém um subconjunto dos dados.\n","\n","As partições são distribuídas pelos nós do cluster, onde são processadas pelos executores. A distribuição das partições permite que o Spark processe os dados em paralelo, melhorando o desempenho e a escalabilidade.\n","\n","Ao criar um RDD, é possível especificar o número de partições desejadas. Se o número de partições não for especificado, o Spark usará o valor padrão, que normalmente é baseado no número de núcleos disponíveis no cluster. O número adequado de partições depende do tamanho dos dados e dos recursos do cluster. É importante escolher um número de partições que equilibre a utilização de recursos e minimize a sobrecarga de comunicação entre as partições.\n","\n","Os RDDs podem ser reparticionados para alterar o número de partições ou o esquema de particionamento. Operações como ```repartition``` e ```coalesce``` podem ser usadas para criar um novo RDD com um número diferente de partições. O reparticionamento pode ser útil para otimizar o desempenho e a utilização de recursos do cluster, especialmente quando a distribuição de dados muda ao longo do tempo.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"SnVNTLg7Fnbd"}},{"cell_type":"markdown","source":["### [coalesce(numPartitions, shuffle=False)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.coalesce.html#pyspark.RDD.coalesce)\n","A função coalesce é usada para reduzir o número de partições em um RDD. Isso é útil quando você deseja otimizar a quantidade de recursos alocados ou quando o processamento em um RDD resulta em um conjunto de dados muito menor do que o original. A função coalesce tenta minimizar a quantidade de dados que precisam ser movidos entre os nós do cluster, mas se shuffle for definido como True, ocorrerá um embaralhamento completo dos dados para redistribuí-los entre as partições."],"metadata":{"id":"6ZDc879AVXy8"}},{"cell_type":"code","source":["data = sc.parallelize([1, 2, 3, 4, 5], 4)\n","\n","print(\"Número de partições antes do coalesce: \", data.getNumPartitions())\n","\n","# Reduzindo o número de partições para 2\n","data = data.coalesce(2)\n","\n","print(\"Número de partições após o coalesce: \", data.getNumPartitions())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6UnYhA13GZXz","executionInfo":{"status":"ok","timestamp":1682475509884,"user_tz":180,"elapsed":409,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"90ea1c09-348e-4a7c-f6a3-0268da5f7870"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["Número de partições antes do coalesce:  4\n","Número de partições após o coalesce:  5\n"]}]},{"cell_type":"markdown","source":["### [repartition(numPartitions)](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.repartition.html#pyspark.RDD.repartition)\n","A função repartition é usada para dividir um RDD em um número específico de partições. Isso é útil quando você deseja aumentar ou diminuir o número de partições de um RDD para otimizar o processamento paralelo."],"metadata":{"id":"_mIcuB8EQ8Pj"}},{"cell_type":"code","source":["data = range(1, 101)\n","rdd = sc.parallelize(data, 2) # Cria um RDD com 2 partições\n","\n","# Verifica o número de partições do RDD original\n","num_partitions_original = rdd.getNumPartitions()\n","print(f\"umero de partições no rdd original: {num_partitions_original}\")\n","\n","# Utiliza a função repartition para criar um novo RDD com 4 partições\n","rdd_repartitioned = rdd.repartition(4)\n","\n","# Verifica o número de partições do RDD reparticionado\n","num_partitions_repartitioned = rdd_repartitioned.getNumPartitions()\n","print(f\"NUmero de partições no RDD reparticionado: {num_partitions_repartitioned}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jci26G49Q_kr","executionInfo":{"status":"ok","timestamp":1682475490623,"user_tz":180,"elapsed":853,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"79c85a53-0dd9-49ec-ebbb-25eaa2add708"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["umero de partições no rdd original: 2\n","NUmero de partições no RDD reparticionado: 4\n"]}]},{"cell_type":"markdown","source":["\n","## Cache de RDDs"],"metadata":{"id":"HvXqKKsjY3Wd"}},{"cell_type":"markdown","source":["Podemos armazenar um RDD intermediário na memória ou no disco para reutilização em várias etapas de processamento. Isso pode melhorar significativamente o desempenho, especialmente em casos de uso iterativos ou quando os mesmos dados são usados várias vezes."],"metadata":{"id":"ehtUOjOdWJ4t"}},{"cell_type":"markdown","source":["### [persist(storageLevel)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.persist.html#pyspark.RDD.persist)\n","A função persist é usada para armazenar em cache um RDD com um nível de armazenamento específico, como memória ou disco. Isso ajuda a melhorar o desempenho quando um RDD é reutilizado em várias operações. O nível de armazenamento pode ser importado do módulo pyspark.StorageLevel."],"metadata":{"id":"MN10UQwzUFUQ"}},{"cell_type":"code","source":["from pyspark import StorageLevel\n","\n","data = sc.parallelize([1, 2, 3, 4, 5])\n","\n","# Persistindo o RDD na memória e no disco\n","data.persist(StorageLevel.MEMORY_AND_DISK)\n","\n","# Reutilizando o RDD em várias operações\n","data.map(lambda x: x * 2).collect()\n","data.filter(lambda x: x % 2 == 0).collect()\n"],"metadata":{"id":"eiNG_hyyUG-h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682475746247,"user_tz":180,"elapsed":1300,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"d9c5cd8c-3e54-4bb9-d979-bdd6c6adcd77"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 4]"]},"metadata":{},"execution_count":84}]},{"cell_type":"markdown","source":["### [cache()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.cache.html#pyspark.RDD.cache)\n","A função cache() é um atalho para persist() com o nível de armazenamento padrão StorageLevel.MEMORY_ONLY. Armazena o RDD na memória para que possa ser reutilizado em várias operações sem a necessidade de recálculo."],"metadata":{"id":"f-d3vyKXUHV8"}},{"cell_type":"code","source":["data = sc.parallelize([1, 2, 3, 4, 5])\n","\n","data.cache()\n","\n","#Reutilizando o RDD em várias operações\n","data.map(lambda x: x * 2).collect()\n","data.filter(lambda x: x % 2 == 0).collect()"],"metadata":{"id":"wJx9w655UQ5W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682475784397,"user_tz":180,"elapsed":1361,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"2be529be-07b5-443e-de3c-9ecfcc0009b6"},"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 4]"]},"metadata":{},"execution_count":85}]},{"cell_type":"markdown","source":["Os dados são armazenados na primeira vez que uma ação é executada no RDD. Isso permite que o Spark otimize a execução e armazene apenas os dados necessários."],"metadata":{"id":"NhIzD-pQWSmP"}},{"cell_type":"markdown","source":["\n","## Conclusão da sessão"],"metadata":{"id":"IkUPI95kY6TS"}},{"cell_type":"markdown","source":["1. Transformações RDD Avançadas\n","2. Estratégia de Particionamento\n","3. Cache de RDDs\n","\n"],"metadata":{"id":"s3RQ79xDY7o9"}},{"cell_type":"markdown","source":["## Exercício"],"metadata":{"id":"NCnmlbdJaB0S"}},{"cell_type":"markdown","source":["1. Dado o seguinte RDD de números: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], use a função flatMap() para criar um novo RDD que contenha o valor original e seu quadrado. Por exemplo, o resultado deve ser: [1, 1, 2, 4, 3, 9, ...]. Verifique a saída usando a ação collect().\n","\n","2. Considere dois RDDs: rdd1 = sc.parallelize([('apple', 3), ('banana', 5), ('orange', 2)]) e rdd2 = sc.parallelize([('apple', 4), ('banana', 2), ('orange', 1)]). Use a função join() para criar um novo RDD que contenha a soma das quantidades para cada fruta, e verifique a saída usando a ação collect().\n","\n","3. Utilize o RDD de pares chave-valor: [('apple', 3), ('banana', 5), ('orange', 2), ('apple', 4)]. Aplique a função aggregateByKey() para calcular a quantidade média de cada fruta e verifique a saída usando a ação collect().\n","\n","4. Dado o RDD: data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 5). Aplique a função coalesce() para reduzir o número de partições para 3 e verifique o número de partições usando a função getNumPartitions().\n","\n","5. Crie um RDD com os números de 1 a 10 e armazene-o em cache usando a função cache(). Em seguida, aplique a transformação map() para elevar cada número ao quadrado e execute a ação collect() para obter a saída resultante. Depois disso, aplique a transformação filter() para obter apenas os números pares e execute a ação collect() novamente."],"metadata":{"id":"uRc2Gm8iJpon"}}]}