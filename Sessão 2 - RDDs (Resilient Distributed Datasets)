{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFeXDGJmObm2ouOS7aapi7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["###### Inicialização"],"metadata":{"id":"lJeDRY2Zc6Tn"}},{"cell_type":"code","source":["# instalar as dependências\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n","!tar xf spark-3.4.0-bin-hadoop3.tgz\n","!pip install -q findspark"],"metadata":{"id":"ty_kN91oadtM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configurar as variáveis de ambiente\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\"\n","\n","# tornar o pyspark \"importável\"\n","import findspark\n","findspark.init('spark-3.4.0-bin-hadoop3')"],"metadata":{"id":"GMGdL4xyc423"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PySpark (Sessão 2)\n","\n"],"metadata":{"id":"frFI5O7cZ5L8"}},{"cell_type":"markdown","source":["![img](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/250px-Apache_Spark_logo.svg.png)\n","\n","\n","https://spark.apache.org/docs/latest/api/python/"],"metadata":{"id":"pJCODccgTiQD"}},{"cell_type":"markdown","source":["## Recapitulando o que vimos antes..."],"metadata":{"id":"bGI5IDjEL5Z7"}},{"cell_type":"markdown","source":["1. Apresentação do Curso\n","2. Introdução ao PySpark\n","3. Primeiros Passos com PySpark"],"metadata":{"id":"kxbx8v0OMg4t"}},{"cell_type":"markdown","source":["## RDDs em PySpark"],"metadata":{"id":"jNLFAvHPMgu0"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","sc = SparkSession.builder.master('local[*]').getOrCreate()"],"metadata":{"id":"kKQv82Uk8NB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# download do http para arquivo local\n","!wget --quiet --show-progress http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2023-03-28/visualisations/listings.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6Vqinct8j-O","executionInfo":{"status":"ok","timestamp":1682471048427,"user_tz":180,"elapsed":1537,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"2d22fee4-019f-4832-eb5e-fc7840e5ded3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["listings.csv        100%[===================>]   4.37M  8.68MB/s    in 0.5s    \n"]}]},{"cell_type":"markdown","source":["### Introdução aos Resilient Distributed Dataset (RDDs)"],"metadata":{"id":"vVKbNefpMgqL"}},{"cell_type":"markdown","source":["Um RDD (Resilient Distributed Dataset) é uma das principais abstrações de dados no PySpark e no Apache Spark. É uma coleção imutável e distribuída de objetos que pode ser processada em paralelo em um cluster. O RDD é projetado para ser tolerante a falhas e possui várias características importantes:\n","\n","1. **Imutabilidade**: Uma vez criado, um RDD não pode ser alterado. Todas as transformações aplicadas a um RDD resultam na criação de um novo RDD, mantendo a imutabilidade. Essa característica facilita o gerenciamento e a recuperação de falhas em aplicações distribuídas.\n","\n","2. **Distribuição**: Os RDDs são distribuídos pelos nós do cluster e divididos em partições. Cada partição é uma fração do RDD que pode ser processada em paralelo por um executor. A distribuição permite que o Spark processe grandes volumes de dados em paralelo, melhorando o desempenho e a escalabilidade.\n","\n","3. **Tolerância a falhas**: Os RDDs são resilientes a falhas porque mantêm um registro das transformações aplicadas a eles, chamado linhagem. Se uma partição de um RDD for perdida devido a uma falha de nó, o Spark pode recriar a partição usando a linhagem e as transformações aplicadas ao RDD original.\n","\n","4. **Operações**: Existem dois tipos principais de operações que podem ser aplicadas aos RDDs: transformações e ações. Transformações são operações que criam um novo RDD a partir de um existente, como map, filter e reduceByKey. Ações são operações que retornam um valor ao driver ou gravam dados em um sistema de armazenamento externo, como count, collect e saveAsTextFile.\n","\n","5. **Persistência**: Os RDDs podem ser persistidos ou armazenados em cache na memória ou no disco para serem reutilizados em várias etapas de processamento. Isso pode melhorar significativamente o desempenho, especialmente em casos de uso iterativos, como algoritmos de machine learning.\n","\n","6. **Computação Lazy**: As transformações aplicadas aos RDDs são computadas de maneira \"lazy\", o que significa que elas não são executadas imediatamente. A execução é adiada até que uma ação seja chamada, permitindo que o Spark otimize e planeje a execução de várias transformações de maneira eficiente.\n","\n","Embora o RDD seja uma das principais abstrações de dados do PySpark, o uso de DataFrames e Datasets (uma extensão tipada de DataFrames) tornou-se mais comum devido à sua facilidade de uso e otimizações adicionais de desempenho. No entanto, os RDDs continuam sendo uma opção poderosa e flexível para cenários de processamento de dados que exigem um controle mais fino sobre a distribuição e a transformação de dados."],"metadata":{"id":"BQYr440T85R1"}},{"cell_type":"code","source":["airbnb_rj = sc.read.csv(\"./listings.csv\", inferSchema=True, header=True).rdd\n","airbnb_rj.take(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kypnQGRc8lhc","executionInfo":{"status":"ok","timestamp":1682471486863,"user_tz":180,"elapsed":14586,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"4f0e0fb3-a3ba-47c8-fcdb-e94a641657d9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(id='17878', name='Very Nice 2Br in Copacabana w. balcony, fast WiFi', host_id='68997', host_name='Matthias', neighbourhood_group=None, neighbourhood='Copacabana', latitude='-22.96599', longitude='-43.1794', room_type='Entire home/apt', price='350', minimum_nights='5', number_of_reviews='288', last_review='2023-03-01', reviews_per_month='1.86', calculated_host_listings_count='1', availability_365='264', number_of_reviews_ltm=19.0, license=None)]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["## limpando dados\n","airbnb_rj = airbnb_rj.filter(lambda x: (x[2] != None) and (x[2].isnumeric()))"],"metadata":{"id":"fgxSquBXqWHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### parallelize(data, numPartitions=None)\n","A função parallelize é usada para criar um RDD a partir de uma coleção de dados existente, como uma lista ou uma matriz. O RDD resultante terá seus elementos distribuídos entre várias partições, que serão processadas em paralelo. O parâmetro numPartitions permite especificar o número de partições a serem criadas. Se numPartitions não for fornecido, o Spark escolherá um valor padrão com base no número de núcleos disponíveis nos nós do cluster."],"metadata":{"id":"qnJz59jYYNx8"}},{"cell_type":"code","source":["# Criando um RDD a partir de uma lista com 3 partições\n","data = sc.sparkContext.parallelize([1, 2, 3, 4, 5], 3)\n","\n","print(\"Número de partições: \", data.getNumPartitions())"],"metadata":{"id":"Ai7PgDJ4YS12","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682471760553,"user_tz":180,"elapsed":19,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"b5ef2ffb-856c-460b-9cc6-99e7f271aba2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Número de partições:  3\n"]}]},{"cell_type":"markdown","source":["### Características e funcionalidades dos RDDs"],"metadata":{"id":"OuSW3gatMglM"}},{"cell_type":"markdown","source":["As principais funcionalidades dos RDDs são:\n","\n","1. **Transformações**: Os RDDs oferecem várias operações de transformação que permitem modificar e criar novos RDDs a partir de um RDD existente. Algumas transformações comuns incluem:\n","  - map: Aplica uma função a cada elemento do RDD e retorna um novo RDD.\n","  - filter: Filtra os elementos do RDD com base em uma condição e retorna um novo RDD.\n","  - flatMap: Aplica uma função a cada elemento do RDD, que retorna uma sequência de zero ou mais itens, e então retorna um novo RDD com todos os itens das sequências.\n","  - union: Retorna um novo RDD que contém a união de elementos de dois RDDs.\n","  - distinct: Retorna um novo RDD com elementos distintos do RDD original.\n","  - groupByKey: Agrupa os elementos do RDD com base em suas chaves.\n","  - reduceByKey: Agrupa os elementos do RDD com base em suas chaves e aplica uma função de redução aos valores de cada grupo.\n","\n","2. **Ações**: Os RDDs oferecem várias operações de ação que retornam um valor ao driver ou gravam dados em um sistema de armazenamento externo. Algumas ações comuns incluem:\n","  - count: Retorna o número de elementos no RDD.\n","  - collect: Retorna todos os elementos do RDD como uma lista (usado com cuidado, pois pode causar problemas de memória se o RDD for grande).\n","  - first: Retorna o primeiro elemento do RDD.\n","  - take: Retorna os primeiros n elementos do RDD como uma lista.\n","  -  reduce: Agrega os elementos do RDD usando uma função de redução e retorna o resultado.\n","  - saveAsTextFile: Salva os elementos do RDD como um arquivo de texto no sistema de armazenamento especificado.\n","\n","3. **Persistência**: Os RDDs podem ser persistidos ou armazenados em cache na memória ou no disco para serem reutilizados em várias etapas de processamento, melhorando o desempenho em casos de uso iterativos ou quando os mesmos dados são usados várias vezes.\n","\n","4. **Particionamento**: Os RDDs são divididos em partições, que são unidades básicas de processamento paralelo. O Spark permite controlar o número de partições e o esquema de particionamento para otimizar a distribuição e o processamento de dados em um cluster.\n","\n","5. **Tolerância a falhas**: Os RDDs são tolerantes a falhas, pois mantêm um registro de linhagem das transformações aplicadas. Isso permite que o Spark recrie partições perdidas devido a falhas de nó ou outros problemas, garantindo a recuperação rápida e a continuidade do processamento de dados.\n","\n","6. **Computação Lazy**: Os RDDs utilizam a computação lazy para adiar a execução de transformações até que uma ação seja chamada. Isso permite que o Spark otimize e planeje a execução de várias transformações de maneira eficiente, melhorando o desempenho geral."],"metadata":{"id":"5ZH712PL--CF"}},{"cell_type":"markdown","source":["<img src=\"https://www.oreilly.com/api/v2/epubs/9781492082378/files/assets/daws_0109.png\" width=\"920px\"/>"],"metadata":{"id":"Gsq58GTp9ogT"}},{"cell_type":"markdown","source":["## Operações de Transformação"],"metadata":{"id":"bQCTcxJIMgWF"}},{"cell_type":"markdown","source":["#### [map(func)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html#pyspark.RDD.map)\n","A função map é uma transformação que aplica uma função func a cada elemento do RDD e retorna um novo RDD com os resultados. Por exemplo, você pode usar map para elevar cada número de um RDD ao quadrado ou converter todas as letras maiúsculas em minúsculas."],"metadata":{"id":"HcxrcZcjAtGg"}},{"cell_type":"code","source":["precos_rj = airbnb_rj.map(lambda x : x[9])\n","precos_rj"],"metadata":{"id":"DBZ1aX8SAU-l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682472000300,"user_tz":180,"elapsed":463,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"384e6ead-efc0-49e5-dd6e-645cd96648f5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PythonRDD[17] at RDD at PythonRDD.scala:53"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["#### [filter(func)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html#pyspark.RDD.filter)\n","A função filter é outra transformação que aplica uma função func a cada elemento do RDD e retorna um novo RDD contendo apenas os elementos que atendem a uma condição especificada. Por exemplo, você pode usar filter para obter apenas números pares ou palavras que começam com uma letra específica."],"metadata":{"id":"Hz8cONfQAv45"}},{"cell_type":"code","source":["#precos_rj = airbnb_rj.filter(lambda x: (x[9] != None) and (x[9].isnumeric())).map(lambda x : int(x[9]))\n","precos_rj = airbnb_rj.map(lambda x : int(x[9])).filter(lambda x: x > 1000)\n","precos_rj.take(1)"],"metadata":{"id":"LcSnoAkHAj5J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682472082252,"user_tz":180,"elapsed":341,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"4bf75390-c5ff-401f-c234-9acffdd27096"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2270]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["#### [distinct(numPartitions=None)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.distinct.html)\n","A função distinct é uma transformação que retorna um novo RDD contendo apenas elementos únicos, removendo duplicatas. Você pode especificar o número de partições no novo RDD usando o parâmetro numPartitions."],"metadata":{"id":"jJbooBPJBC1Q"}},{"cell_type":"code","source":["airbnb_rj.map(lambda x : x[0]).distinct()"],"metadata":{"id":"OyW6aIWRAnFJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682472126181,"user_tz":180,"elapsed":373,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"c752c556-86f1-4a78-cf1b-d93d31add654"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PythonRDD[23] at RDD at PythonRDD.scala:53"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["#### [reduce(func)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html#pyspark.RDD.reduce)\n","A função reduce é uma ação que agrega os elementos do RDD usando uma função func fornecida que recebe dois argumentos e retorna um único valor. A função deve ser comutativa e associativa para garantir que possa ser aplicada corretamente em qualquer ordem. Por exemplo, você pode usar reduce para somar todos os elementos de um RDD ou encontrar o valor máximo."],"metadata":{"id":"6TV5V8V4BOvm"}},{"cell_type":"code","source":["from operator import add\n","precos_rj.reduce(add)"],"metadata":{"id":"stYxsmJ7Aqit","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682472145029,"user_tz":180,"elapsed":3685,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"8f73ab00-c6d1-4beb-be16-d94f393b2e95"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20161723"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["#### [groupByKey([numPartitions])](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.groupByKey.html#pyspark.RDD.groupByKey)\n","Retorna um novo RDD de pares (key, values), onde os valores para cada chave são agrupados em uma lista. É importante notar que essa operação pode causar um grande tráfego de dados na rede se as chaves não estiverem distribuídas uniformemente."],"metadata":{"id":"gU7DcnxjBJzZ"}},{"cell_type":"code","source":["bairro_e_preco = airbnb_rj.map(lambda x : (x[5], int(x[9])))\n","bairros_e_precos = bairro_e_preco.groupByKey().map(lambda x : (x[0], list(x[1])[:5])) #5 primeiros preços\n","\n","bairros_e_precos.take(5)"],"metadata":{"id":"Nj5oN_PHBVqn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682472288557,"user_tz":180,"elapsed":2049,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"6af7f012-2639-43ba-837e-586b4e3e1077"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Ipanema', [624, 100, 2270, 1899, 497]),\n"," ('Barra da Tijuca', [539, 498, 400, 270, 650]),\n"," ('Flamengo', [780, 650, 1040, 327, 800]),\n"," ('Laranjeiras', [300, 468, 113, 177, 147]),\n"," ('Santa Teresa', [220, 203, 130, 1031, 300])]"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["#### [reduceByKey(func, [numPartitions])](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey.html#pyspark.RDD.reduceByKey)\n","Retorna um novo RDD de pares (key, value), onde os valores para cada chave são reduzidos aplicando a função func. Essa função deve ser uma função que recebe dois argumentos e retorna um único valor."],"metadata":{"id":"sgPPOhZnBSE-"}},{"cell_type":"code","source":["from operator import add\n","\n","bairro_e_preco = airbnb_rj.map(lambda x : (x[5], int(x[9])))\n","bairros_e_soma_precos = bairro_e_preco.reduceByKey(add)\n","bairros_e_soma_precos"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wmBshcGHkVQN","executionInfo":{"status":"ok","timestamp":1682472328968,"user_tz":180,"elapsed":676,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"2c2869b9-842f-4a26-f722-77fe6f214216"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PythonRDD[39] at RDD at PythonRDD.scala:53"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["bairros_e_soma_precos.take(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"619t2Zzbo9xA","executionInfo":{"status":"ok","timestamp":1682472336482,"user_tz":180,"elapsed":1665,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"87d77a25-c277-4a1f-f056-39480be578c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Ipanema', 2855061),\n"," ('Barra da Tijuca', 3609489),\n"," ('Flamengo', 340090),\n"," ('Laranjeiras', 201624),\n"," ('Santa Teresa', 664986)]"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["## Operações de Ação"],"metadata":{"id":"wit8N4ztMgGy"}},{"cell_type":"markdown","source":["#### [take(n)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.take.html#pyspark.RDD.take)\n","A função take é uma ação que retorna os primeiros n elementos do RDD como uma lista. É útil quando você deseja inspecionar rapidamente os dados ou testar seu código em um pequeno subconjunto de dados."],"metadata":{"id":"hbFWDqpKBvGY"}},{"cell_type":"code","source":["bairros_e_soma_precos.take(10)"],"metadata":{"id":"S05lB8tuB2na","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682472415395,"user_tz":180,"elapsed":434,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"43f181ca-bc40-462c-e88d-efd38d095cde"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Ipanema', 2855061),\n"," ('Barra da Tijuca', 3609489),\n"," ('Flamengo', 340090),\n"," ('Laranjeiras', 201624),\n"," ('Santa Teresa', 664986),\n"," ('Centro', 336452),\n"," ('São Conrado', 591820),\n"," ('Recreio dos Bandeirantes', 1611444),\n"," ('Santo Cristo', 9936),\n"," ('Glória', 207323)]"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["#### [collect()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html#pyspark.RDD.collect)\n","A função collect é uma ação que retorna todos os elementos do RDD como uma lista. Essa função deve ser usada com cautela, pois pode causar problemas de memória se o RDD for muito grande. É útil principalmente para testes e depuração."],"metadata":{"id":"1F_zvYUPBfqx"}},{"cell_type":"code","source":["bairros_e_soma_precos.collect()[:10]"],"metadata":{"id":"rwJ3-fCDB1Gu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682472460422,"user_tz":180,"elapsed":842,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"15ef592b-3880-474d-f5a5-0030b8cb38b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Ipanema', 2855061),\n"," ('Barra da Tijuca', 3609489),\n"," ('Flamengo', 340090),\n"," ('Laranjeiras', 201624),\n"," ('Santa Teresa', 664986),\n"," ('Centro', 336452),\n"," ('São Conrado', 591820),\n"," ('Recreio dos Bandeirantes', 1611444),\n"," ('Santo Cristo', 9936),\n"," ('Glória', 207323)]"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["#### [count()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html#pyspark.RDD.count)\n","A função count é uma ação que retorna o número de elementos no RDD. É útil para verificar o tamanho do conjunto de dados após a aplicação de transformações como filter ou distinct."],"metadata":{"id":"QPhkcRbQBrsY"}},{"cell_type":"code","source":["bairros_e_soma_precos.count()"],"metadata":{"id":"t_78hPeaB1uY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682472467527,"user_tz":180,"elapsed":643,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"8a051c90-248d-4fd9-c845-7711e26478b7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["153"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["#### [first()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.first.html#pyspark.RDD.first)\n","A função first é uma ação que retorna o primeiro elemento do RDD. É semelhante a take(1), mas retorna apenas um único valor em vez de uma lista."],"metadata":{"id":"zxjs8go9Btp7"}},{"cell_type":"code","source":["bairros_e_soma_precos.first()"],"metadata":{"id":"NCfWffMfB2SS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682472484979,"user_tz":180,"elapsed":1356,"user":{"displayName":"Luan Corumba","userId":"07892592111474770617"}},"outputId":"089d30d5-7b9f-4e91-ce92-841dcead68ac"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('Ipanema', 2855061)"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["#### [saveAsTextFile(path, compressionCodecClass=None)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.saveAsTextFile.html#pyspark.RDD.saveAsTextFile)\n","A função saveAsTextFile é uma ação que salva o RDD como um arquivo de texto no caminho especificado. O RDD é convertido em uma sequência de strings, com cada elemento sendo separado por uma nova linha. O parâmetro opcional compressionCodecClass permite especificar uma classe de codec de compressão para comprimir os dados de saída."],"metadata":{"id":"p-DxErsWBxAC"}},{"cell_type":"code","source":["bairros_e_soma_precos.saveAsTextFile(\"./bairros_e_soma_precos\")"],"metadata":{"id":"m-8NjQlfB3KQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!rm -r ./bairros_e_soma_precos"],"metadata":{"id":"bBdr_Ttix5Hr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Conclusão da sessão"],"metadata":{"id":"c20KdeYiMf2B"}},{"cell_type":"markdown","source":["1. Introdução aos RDDs\n","2. Operações de Transformação\n","3. Operações de Ação"],"metadata":{"id":"oD932sfTMe1-"}},{"cell_type":"markdown","source":["### Onde aplicamos\n","\n","Os RDDs são especialmente úteis em aplicações que exigem processamento distribuído e paralelo de grandes volumes de dados, como **análise de logs, processamento de dados de sensores, pré-processamento e limpeza de dados, análise de redes sociais, detecção de anomalias e algoritmos de aprendizado de máquina iterativos**. Ao utilizar RDDs, os desenvolvedores podem tirar proveito da escalabilidade, tolerância a falhas e flexibilidade oferecidas pelo Apache Spark para criar aplicações robustas e eficientes, capazes de lidar com os desafios de big data em diversos setores e domínios."],"metadata":{"id":"WqrBlB5EDjGi"}},{"cell_type":"markdown","source":["## Exercício"],"metadata":{"id":"NCnmlbdJaB0S"}},{"cell_type":"markdown","source":["1. Crie um RDD usando a função parallelize() com a seguinte lista de números: [2, 4, 6, 8, 10]. Depois, aplique uma transformação map() para elevar cada número ao quadrado e, em seguida, execute uma ação collect() para obter a saída resultante.\n","\n","2. Dado o seguinte RDD de palavras: ['Python', 'Scala', 'Java', 'R', 'C++'], use a função filter() para criar um novo RDD que contenha apenas palavras com mais de três caracteres. Verifique a saída usando a ação collect().\n","\n","3. Considere dois RDDs: rdd1 = sc.parallelize([1, 2, 3, 4, 5]) e rdd2 = sc.parallelize([4, 5, 6, 7, 8]). Realize as seguintes operações:\n","  - Calcule a união desses dois RDDs e exiba o resultado.\n","  - Calcule a interseção desses dois RDDs e exiba o resultado.\n","  - Realize a operação subtract() para remover os elementos de rdd1 presentes em rdd2 e exiba o resultado.\n","\n","4. Usando o seguinte RDD de pares chave-valor: [('apple', 3), ('banana', 5), ('orange', 2), ('apple', 4)], execute as seguintes operações:\n","  - Agrupe os elementos pelo nome da fruta usando groupByKey() e calcule a soma das quantidades para cada fruta. Verifique a saída usando collect().\n","  - Calcule a quantidade total de frutas usando a função reduceByKey() e verifique a saída usando collect().\n","\n","4. Crie um RDD com 100 números inteiros aleatórios entre 1 e 1000. Depois, use a função sortBy() para ordenar os números em ordem decrescente e exiba os 10 maiores números usando a ação take()."],"metadata":{"id":"uRc2Gm8iJpon"}}]}